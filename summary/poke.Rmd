---
title: "USA Real Estate Data"
author: "Team 6"
date: "10/17/2022"
output:  
    rmdformats::downcute:
      toc_float: true
      number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(scipen = 999, digits = 3, big.mark=",", warn = -1)
```

```{r init, include=FALSE}
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
 
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "show", message = F)
options(scientific=T, digits = 3) 
options(scipen=999, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```
```{r include=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(corrplot)
```

```{r include=FALSE}
df <- data.frame(read.csv("houseprices.csv"))
```

# Introduction

The USA has one of the hottest real estate markets right now. We needed a massive amount of data with various factors to know and analyze the real-estate data. Forecasting and evaluating the real estate market helps assess the stability of the US real estate market. Predicting home prices may also give a solid foundation for real estate investors to develop investment strategies and minimize losses. The issue arises when numerous variables, such as location and property demand, may influence the house price. Thus most stakeholders, including buyers and developers, home builders, and the real estate sector, would like to know the exact attributes or the specific factors that influence house prices to help customers and investors make decisions.

To get all this data, we got a scraped dataset from the website https://www.realtor.com (found on Kaggle). This dataset contains features like no. of beds, baths, streets, city, and zip code and has 923,160 observations. To get meaningful insights into this massive pile of data, we have to perform statistical analysis, Exploratory Data Analysis on the data set and visualize some of the observations.

This summary report is organised as follows - 

1. Description of the data
2. EDA: Insights from Visualization
3. 

# Description of the Data

## Original Data

The dataset named "houseprices.csv" contains 923160 values and 12 variables. More details are shown below-

* status: Whether the house is sold or not.

* price: The price of the house

* bed: No. of beds in the house

* bath: No. of baths in the house

* acre_lot: The lot size in acres

* full_address: The address of the house

* street: The full address of the house spliced upto the street name.

* city: The full address of the house spliced only to the city.

* state: The state in which the house is located

* zip_code: The zipcode of the area in which the house is located

* house_size: The size of the house in sq.ft.

* sold_date: Mostly a null_value column as most of the houses are not sold yet.

The least price in the dataset is `r min(df$price)` while the highest price is `r max(df$price)`. This means the there is much variance between the data. The mean of the price is `r round(mean(df$price))` and the standard deviation is `r  round(sd(df$price))`. The variance is `r round(var(df$price))`. From the variance, we can see that the data is distributed very widely and spread out from the mean.

## Preprocessing:
Our dataset is ready for preprocessing, so we executed the folloing steps:

1. We checked the number of null values and duplicated values in the dataset and suprisingly, it was 84% of our dataset.

2. We then tried to replace the null values with the mean and median, but that simply killed the insights from our data as it is real-world and simply doesn't work in the way we expected.

3. Then we removed the following columns: full_address, street, zip_code, sold_date. This was done because these variables in no way affect the output from the data and the correlation was also not great.

4. Then we converted the bed, bath into factor variables from integer variables so that they can work as intervals.

5. Finally we were left with `r nrow(df)` rows of data.

6. So our data is finally preprocessed and ready to perform EDA on.

```{r echo=FALSE, results=T}
print(summary(df))
```

# EDA
To perform EDA in our dataset, we first need to know the relationship between our dependent and independent variables. So we built some graphs between our dependent(Price) and independent variables(everything else).

## Visualization

First, we need to know what the distribution of the variables. So, we created a graph which shows the frequency of the house size:

```{r echo=FALSE, results=T}
ggplot(df, aes(house_size)) + geom_histogram(binwidth = 20, col = 'light green' ) + labs(title = "Frequency of the house sizes")
```
Here, we can understand that most of the houses in the dataset are medium sized houses which are in the market. So, to check the acre _lot for these medium-sized houses, we create a similar histogram for the dataset.
```{r echo=FALSE, results=T}
ggplot(df, aes(acre_lot)) + geom_histogram(binwidth = 0.008, col = 'light green' ) + labs(title = "Frequency of the acre lot")
```
Here we can see that even for medium-sized houses, the acre_lot is very low. This means that as long as people have parking for one car and a small lawn, the house can be popular in the market.

Next, we need to analyze the relationship between dependent and independent variables. First, we need to see the relationship between house size and the price of the house.

```{r echo=FALSE, results=T}
ggplot(df, aes(x = house_size, y = price)) + geom_bar(stat = "identity", width = 80, col = 'light green') + labs(title = 'Price vs House_size')
```
We can observe that even though the size of the house is increasing drastically, the price of the house is not increasing in tandem to the size of the house. This means that there are some more significant factors which affect the price of the house.

To find out these factors, we did the bar plot between the dependent variable and all the independent variables.

```{r echo=FALSE, results=T}
ggplot(df, aes(x = bath, y = price)) + geom_bar(stat = "identity", width = 0.6, col = 'light green') + labs(title = 'Price vs No. of bathrooms')
```
From this graph, we can see that the highest priced houses are mostly found with 3 bathrooms and 4 bathroom houses are generally priced less compared to others. This shows that as long as people have 2 or 3 bathrooms, the house is hugely popular. But, even this graph does not explain why the 4 bathroom houses are priced so low.

Next, the graph is between no. of bedrooms and the price of the house.

```{r echo=FALSE, results=T}
ggplot(df, aes(x = bed, y = price)) + geom_bar(stat = "identity", width = 0.7, col = 'light green') + labs(title = 'Price vs No. of beds')
```
Even this graph tells the same thing that the highest priced houses have 3 to 4 bedrooms and houses wiht 6, 7 bedrooms are priced horrifyingly low.

Now, only independent variable left is the area of the house. So, to plot the graph between price of the house and the area of the house.

```{r echo=FALSE, results=T}
ggplot(df, aes(x = state, y = price)) + geom_bar(stat = "identity", width = 0.8, col = 'light green') + labs(title = 'Price vs House_size') + theme(axis.text.x = element_text(angle = 90))
```
Most of our confusion was explained from this graph. As the sizes of house got bigger, this means that they were in poorly populated areas. So, they were either in the suburbs or places where the general price of the houses is low. Some of the states were Maine, a part of Virgin Islands etc. From this rationale, we can analyze that big cities like New York won't have big houses as they are heavily over crowded and even small houses are over-priced.

Now, we wanted to find out the mathematical values such as mean, median, standard deviation for each state separately. So we used the dplyr function to group the data by states and calcuate the values.

```{r echo=FALSE, results=T}
stats<-data.frame(df %>% group_by(state) %>% summarise(mean = mean(price), median = median(price), max= max(price), min = min(price), sd = sd(price)))
```

First we visualized the data between the mean price and the state.
```{r echo=FALSE, results=T}
ggplot(stats, aes(x = state, y = mean)) + geom_bar(stat = "identity", col = 'light blue', fill = 'light blue') + theme(axis.text.x = element_text(angle=90)) + labs(title = 'Mean prices in different states')
```
This shows that houses in New York has the highest mean of houses. This means the range of the price of the houses is really high. This is followed by Virgin Islands. In the previous graphs, we have seen that Virgin Islands have the least amount of price But from this graph, we can gain that although the price is low, there is a great variation between the cities in which the houses are located and the costliest houses in Virgin Islands are all concentrated in one place. 

Similarly, the graph of the median price in states is as follows:
```{r echo=FALSE, results=T}
ggplot(stats, aes(x = state, y = median)) + geom_bar(stat = "identity", col = 'light blue', fill = 'light blue') + theme(axis.text.x = element_text(angle=90)) + labs(title = 'Median prices in different states')
```
From the median of prices of different states, we can see that the highest median price is in the New York City. This shows us how much the prices in New York really are. 

We also did a similar graph for standard deviation of prices for each state. This will show us the distribution of prices from the mean.

```{r echo=FALSE, results=T}
ggplot(stats, aes(x = state, y = sd)) + geom_bar(stat = "identity", col = 'light blue', fill = 'light blue') + theme(axis.text.x = element_text(angle=90)) + labs(title = 'Standard deviation between prices in different states')
```
From this graph, we can see that the deviation from the mean is the highest in Virgin Islands, where the difference between the lowest price and the highest price is very high. The deviation from the mean is also very high in Virgin Islands, This speaks to the geography of the state.

Now, let's look at the correlation between each variable with price.
```{r echo=FALSE, results=T}
df_corr <- df
df_corr$bed <- as.integer(df_corr$bed)
df_corr$bath <- as.integer(df_corr$bath)
df_corr <- subset(df_corr, select = -c(city,state, zip_code))
corr <- cor(df_corr, method = 'pearson')
corrplot(corr, method='square')
corrplot(corr, method='number')
```
From this correlation matrix, we can see that the house_size and no. of baths has the highest correlation with the price of the house.

## TESTING

```{r}
str(df)
anova_hb <- aov(price ~ bed, data= df)
anova_hb_summary <- summary(anova_hb)
anova_hb_summary
xkabledply(anova_hb)
tukey_test <- TukeyHSD(anova_hb)
tukey_test
plot(TukeyHSD(anova_hb, conf.level=.95))
```

H0: The mean house prices are the same across the different number of the beds.
H1: The mean house prices are different across the different number of the beds.
The anova test for the house price and the number of beds indicates that as p value is less than 0.05. So we need to reject the null hypothesis, and the mean price of the houses are different for the number of beds.
As the means of the groups are significantly different, we can use the Turkey test for possible pairwise comparisons. Expect 6-4 and 7-5 remaining comparisons are statistically significant at 95 percent confidence interval. The Tukey plot describes the difference in the mean values between the different levels of the bed.

```{r}
anova_hbath <- aov(price ~ bath, data= df)
anova_hbath_summary <- summary(anova_hbath)
anova_hbath_summary
xkabledply(anova_hbath)
tukey_test_hbath <- TukeyHSD(anova_hbath)
tukey_test_hbath
plot(TukeyHSD(anova_hbath, conf.level=.95))
```
H0: The mean house prices are the same across the different number of the baths.
H1: The mean house prices are different across the different number of the baths.
The anova test for the house price and the number of bath rooms indicates that as p value is less than 0.05. So we need to reject the null hypothesis, and the mean price of the houses are different form the number of baths. From the tukey test the pair-wise comparison between each group is statistically significant at 95% confidence interval.

```{r}
contable = table(df$bed, df$bath)
xkabledply(contable, title="Contingency table for beds vs baths in house_dataframe")
chitest = chisq.test(contable)
chitest
```
H0: The number of bedrooms and number of bathrooms are independent.
H1: The number of bedrooms and number of bedrooms are not independent.
Since the p-value is 0.0000000000000002, which is lower than 0.05, so we need to reject the null hypothesis. Thus, condition and grade are not independent. Both the bed and bath are correlated.
